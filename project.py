import streamlit as st
import pandas as pd
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.chains.question_answering import load_qa_chain
from langchain.prompts import PromptTemplate
import os

# --- Veri Ä°ÅŸleme Fonksiyonu ---
def preprocess_nba_data(file_path):
    try:
        df = pd.read_csv(file_path, delimiter=';')
    except FileNotFoundError:
        st.error(f"Hata: '{file_path}' dosyasÄ± bulunamadÄ±. LÃ¼tfen dosyanÄ±n doÄŸru yolda olduÄŸundan emin olun.")
        return None, None
    except Exception as e:
        st.error(f"Dosya okunurken bir hata oluÅŸtu: {e}")
        return None, None

    documents = []
    metadata_list = []
    df.columns = df.columns.str.strip()
    
    for index, row in df.iterrows():
        try:
            # Daha detaylÄ± ve aranabilir text oluÅŸtur
            text = (
                f"Oyuncu: {row['Player']}\n"
                f"TakÄ±m: {row['Tm']}\n"
                f"Tarih: {row['Data']}\n"
                f"Rakip: {row['Opp']}\n"
                f"SÃ¼re: {row['MP']} dakika\n"
                f"SayÄ±: {row['PTS']} puan\n"
                f"Ribaund: {row['TRB']}\n"
                f"Asist: {row['AST']}\n"
                f"Top Ã‡alma: {row['STL']}\n"
                f"Blok: {row['BLK']}\n"
                f"Saha Ä°Ã§i BaÅŸarÄ± YÃ¼zdesi: {float(row['FG%'])*100:.1f}%\n\n"
                f"Ã–zet: {row['Player']} ({row['Tm']}), {row['Data']} tarihinde {row['Opp']} takÄ±mÄ±na karÅŸÄ± "
                f"{row['MP']} dakika oynadÄ± ve {row['PTS']} sayÄ±, {row['TRB']} ribaund, {row['AST']} asist kaydetti."
            )
            documents.append(text)
            
            # Metadata ekle
            metadata_list.append({
                'player': row['Player'],
                'team': row['Tm'],
                'date': row['Data'],
                'points': row['PTS']
            })
        except (ValueError, KeyError, TypeError) as e:
            continue
    
    return df, documents, metadata_list

# --- LangChain ve FAISS ile VektÃ¶r VeritabanÄ± OluÅŸturma ---
@st.cache_resource
def create_vector_store(documents, metadata_list):
    try:
        embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",  # TÃ¼rkÃ§e destekli model
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
        
        # Metadata ile birlikte vector store oluÅŸtur
        from langchain.schema import Document
        docs = [Document(page_content=text, metadata=meta) for text, meta in zip(documents, metadata_list)]
        vector_store = FAISS.from_documents(docs, embedding=embeddings)
        
        return vector_store
    except Exception as e:
        st.error(f"VektÃ¶r veritabanÄ± oluÅŸturulurken hata oluÅŸtu: {e}")
        return None

# --- Gemini ve LangChain ile Cevap Ãœretme ---
def get_conversational_chain(api_key):
    prompt_template = """
    Sen bir NBA istatistik asistanÄ±sÄ±n. AÅŸaÄŸÄ±daki oyuncu verileri baÄŸlamÄ±nda sorularÄ± yanÄ±tlÄ±yorsun.
    
    Ã–NEMLÄ° KURALLAR:
    1. Sadece verilen baÄŸlamdaki bilgileri kullan
    2. SayÄ±sal verileri doÄŸru bir ÅŸekilde aktar
    3. Oyuncu isimlerini tam olarak belirt
    4. Tarih bilgilerini ekle
    5. EÄŸer baÄŸlamda bilgi yoksa, aÃ§Ä±kÃ§a "Bu bilgiye sahip deÄŸilim" de
    
    BAÄLAM:
    {context}
    
    SORU: {question}
    
    CEVAP (TÃ¼rkÃ§e ve detaylÄ±):
    """
    
    model = ChatGoogleGenerativeAI(
        model="gemini-2.5-pro", 
        temperature=0.1,  # Daha deterministik cevaplar iÃ§in dÃ¼ÅŸÃ¼k
        google_api_key=api_key,
        convert_system_message_to_human=True
    )
    prompt = PromptTemplate(template=prompt_template, input_variables=["context", "question"])
    chain = load_qa_chain(model, chain_type="stuff", prompt=prompt)
    return chain

# --- Streamlit ArayÃ¼zÃ¼ ---
st.set_page_config(page_title="ğŸ€ NBA Fantezi AsistanÄ±", layout="wide")
st.title("ğŸ€ NBA Fantezi AsistanÄ± Chatbot'u")
st.write("Oyuncu istatistikleri hakkÄ±nda sorularÄ±nÄ±zÄ± sorun!")

# API AnahtarÄ±nÄ± kullanÄ±cÄ±dan al
api_key = st.sidebar.text_input("Google API AnahtarÄ±nÄ±zÄ± Girin:", type="password")

if api_key:
    # Veriyi yÃ¼kle ve iÅŸle
    FILE_PATH = 'nba_fantasy_dataset.csv'
    
    result = preprocess_nba_data(FILE_PATH)
    if result and len(result) == 3:
        df, documents, metadata_list = result
    else:
        df, documents, metadata_list = None, None, None

    if documents:
        st.sidebar.success(f"âœ… {len(documents)} oyuncu verisi yÃ¼klendi")
        
        # VektÃ¶r veritabanÄ±nÄ± oluÅŸtur
        with st.spinner("VektÃ¶r veritabanÄ± oluÅŸturuluyor... (Ä°lk seferde 1-2 dakika sÃ¼rebilir)"):
            vector_store = create_vector_store(documents, metadata_list)
        
        if vector_store:
            st.success("âœ… Sistem hazÄ±r! SorularÄ±nÄ±zÄ± sorabilirsiniz.")
            
            # Ã–rnek sorular gÃ¶ster
            st.sidebar.markdown("### ğŸ’¡ Ã–rnek Sorular:")
            st.sidebar.markdown("""
            - Jayson Tatum kaÃ§ sayÄ± attÄ±?
            - LeBron James'in ribaund sayÄ±sÄ± nedir?
            - En yÃ¼ksek sayÄ±yÄ± kim attÄ±?
            - Boston Celtics oyuncularÄ±nÄ±n performansÄ± nasÄ±l?
            """)
            
            # KullanÄ±cÄ±dan soru al
            user_question = st.text_input(
                "Sorunuzu yazÄ±n:", 
                placeholder="Ã–rnek: 'Jayson Tatum en son maÃ§Ä±nda kaÃ§ sayÄ± attÄ±?'"
            )

            if st.button("ğŸ” Soru Sor"):
                if user_question:
                    with st.spinner("Cevap aranÄ±yor..."):
                        try:
                            # Daha fazla belge getir (k=5)
                            docs = vector_store.similarity_search(user_question, k=5)
                            
                            # Debug: Bulunan belgeleri gÃ¶ster
                            st.info(f"ğŸ“Š {len(docs)} ilgili kayÄ±t bulundu")
                            
                            if not docs:
                                st.warning("âš ï¸ Ä°lgili veri bulunamadÄ±. LÃ¼tfen sorunuzu farklÄ± ÅŸekilde sorun.")
                            else:
                                chain = get_conversational_chain(api_key)
                                response = chain(
                                    {"input_documents": docs, "question": user_question}, 
                                    return_only_outputs=True
                                )
                                
                                st.write("### ğŸ’¬ Cevap:")
                                st.write(response["output_text"])
                                
                                # Kaynak gÃ¶ster
                                with st.expander("ğŸ“š KullanÄ±lan Kaynaklar"):
                                    for i, doc in enumerate(docs, 1):
                                        st.write(f"**Kaynak {i}:**")
                                        st.code(doc.page_content)
                                        if doc.metadata:
                                            st.json(doc.metadata)
                                        st.divider()
                        except Exception as e:
                            st.error(f"Cevap Ã¼retilirken hata oluÅŸtu: {e}")
                            st.exception(e)
                else:
                    st.warning("âš ï¸ LÃ¼tfen bir soru girin.")
        else:
            st.error("VektÃ¶r veritabanÄ± oluÅŸturulamadÄ±.")
    else:
        st.error("âŒ Veri dosyasÄ± yÃ¼klenemedi veya iÅŸlenemedi.")
        st.info("ğŸ’¡ 'nba_fantasy_dataset.csv' dosyasÄ±nÄ±n proje klasÃ¶rÃ¼nde olduÄŸundan emin olun.")
else:
    st.sidebar.warning("âš ï¸ LÃ¼tfen baÅŸlamak iÃ§in Google API anahtarÄ±nÄ±zÄ± girin.")
    st.info("ğŸ‘ˆ Sol taraftaki alana API anahtarÄ±nÄ±zÄ± girin.")